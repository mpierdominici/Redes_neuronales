{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mBlkilaNC2M"
   },
   "outputs": [],
   "source": [
    "# Estos dos comandos evitan que haya que hacer reload cada vez que se modifica un paquete\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "from matplotlib import pyplot as plot\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import csv\n",
    "from scipy.stats import norm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XI-bramQQLk"
   },
   "source": [
    "# Ejercicio de clasificación de texto\n",
    "\n",
    "Naive Bayes es una técnica estadística que consiste en repetir el método anterior en problemas cuyos sucesos no son independientes, pero suponiendo independencia.\n",
    "A lo largo de este trabajo desarrollarán un modelo de Naive Bayes para el problema de clasificación de artículos periodístios.En este caso podemos estimar la probabilidad de ocurrencia de cada palabra según la categoría a la que pertenece el artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJ7IlPHXEDEJ"
   },
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z8adA_beEXG7"
   },
   "source": [
    "El primer paso es obtener el dataset que vamos a utilizar. El dataset a utilizar es el de TwentyNewsGroup(TNG) que está disponible en sklearn.\n",
    "\n",
    "Se puede encontrar más información del dataset en la documentación de scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "AHaexjmQNQej",
    "outputId": "2dd3fd54-8454-4dde-8c08-d2b917679fdf"
   },
   "outputs": [],
   "source": [
    "#Loading the data set - training data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u65YieUTEgR3"
   },
   "source": [
    "El siguiente paso es analizar el contenido del dataset, como por ejemplo la cantidad de artículos, la cantidad de clases, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sx2_w9hbS9-D"
   },
   "source": [
    "Preguntas:\n",
    "\n",
    "1) Cuántos articulos tiene el dataset?\n",
    "\n",
    "2) Cuántas clases tiene el dataset?\n",
    "\n",
    "3) Es un dataset balanceado?\n",
    "\n",
    "4) Cuál es la probabilidad a priori de la clase 5? A que corresponde esta clase?\n",
    "\n",
    "5) Cuál es la clase con mayor probabilidad a priori?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmBRcNNbRvVu"
   },
   "source": [
    "1) Cuántos articulos tiene el dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de articulos\n",
      "11314\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de articulos')\n",
    "numArticulos=len(twenty_train['data'])\n",
    "print(numArticulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Cuántas clases tiene el dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de clases\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print('Cantidad de clases')\n",
    "numClases=len(twenty_train['target_names'])\n",
    "print(numClases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Es un dataset balanceado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidadPorClases=np.zeros(numClases)\n",
    "for i in range(numClases):\n",
    "    cantidadPorClases[i]=len(twenty_train['target'][twenty_train['target']==i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "indexMin=np.argmin(cantidadPorClases)\n",
    "indexMax=np.argmax(cantidadPorClases)\n",
    "\n",
    "if(cantidadPorClases[indexMin]==cantidadPorClases[indexMax]):\n",
    "    print('si')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuál es la probabilidad a priori de la clase 5? A que corresponde esta clase?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase correspondiente a 5 es:\n",
      "comp.windows.x\n",
      "Su probavilidad a priori es 0.05241293972070002\n"
     ]
    }
   ],
   "source": [
    "probApriori5=cantidadPorClases[5]/numArticulos\n",
    "print('La clase correspondiente a 5 es:')\n",
    "print(twenty_train['target_names'][5])\n",
    "print('Su probavilidad a priori es '+str(probApriori5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Cuál es la clase con mayor probabilidad a priori?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La maxima proba a priori correponde a la clase:rec.sport.hockey 10\n"
     ]
    }
   ],
   "source": [
    "print('La maxima proba a priori correponde a la clase:' + twenty_train['target_names'][indexMax]+' '+str(indexMax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84hQvk-UHX1V"
   },
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Para facilitar la comprensión de los algoritmos de preprocesamiento, se aplican primero a un solo artículo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0zxZSOsFXpl"
   },
   "source": [
    "\n",
    "Mas info en:\n",
    "http://text-processing.com/demo/stem/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-geItZPyUWEO"
   },
   "source": [
    "- **Tokenization (nltk):**\n",
    "\n",
    "Dada una secuencia de caracteres, la tokenización consiste en dividir esta subpartes denominadas tokens. Un token puede ser palabras individuales, sílabas, frases o cualquier combinación de ellos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', ':', 'lerxst', '@', 'wam.umd.edu', '(', 'where', \"'s\", 'my', 'thing', ')', 'Subject', ':', 'WHAT', 'car', 'is', 'this', '!', '?', 'Nntp-Posting-Host', ':', 'rac3.wam.umd.edu', 'Organization', ':', 'University', 'of', 'Maryland', ',', 'College', 'Park', 'Lines', ':', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'was', 'a', '2-door', 'sports', 'car', ',', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'was', 'called', 'a', 'Bricklin', '.', 'The', 'doors', 'were', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'is', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'is', 'made', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', ',', 'please', 'e-mail', '.', 'Thanks', ',', '-', 'IL', '--', '--', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '--', '--']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mpier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "tok = word_tokenize(twenty_train['data'][0])\n",
    "print(tok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pksduBGKFMxt"
   },
   "source": [
    "\n",
    "- **Lemmatization (nltk):**\n",
    "\n",
    "Consiste en llevar a las distintas conjugaciones de una palabra a su origen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mpier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', ':', 'lerxst', '@', 'wam.umd.edu', '(', 'where', \"'s\", 'my', 'thing', ')', 'Subject', ':', 'WHAT', 'car', 'be', 'this', '!', '?', 'Nntp-Posting-Host', ':', 'rac3.wam.umd.edu', 'Organization', ':', 'University', 'of', 'Maryland', ',', 'College', 'Park', 'Lines', ':', '15', 'I', 'be', 'wonder', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day', '.', 'It', 'be', 'a', '2-door', 'sport', 'car', ',', 'look', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s', '.', 'It', 'be', 'call', 'a', 'Bricklin', '.', 'The', 'doors', 'be', 'really', 'small', '.', 'In', 'addition', ',', 'the', 'front', 'bumper', 'be', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', '.', 'This', 'be', 'all', 'I', 'know', '.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'of', 'production', ',', 'where', 'this', 'car', 'be', 'make', ',', 'history', ',', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'look', 'car', ',', 'please', 'e-mail', '.', 'Thanks', ',', '-', 'IL', '--', '--', 'bring', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '--', '--']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem=[lemmatizer.lemmatize(x,pos='v') for x in tok]\n",
    "print(lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-zxqqFYAFOsN"
   },
   "source": [
    "- **Stop Words (nltk):**\n",
    "\n",
    "Uno de los mayores objetivos del pre procesamiento es remover los datos que no aportan información. Las palabras que no aportan información suelen llamarse Stop Words. Estos pueden ser palabras como un, una, la, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mpier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From', ':', 'lerxst', '@', 'wam.umd.edu', '(', \"'s\", 'thing', ')', 'Subject', ':', 'WHAT', 'car', '!', '?', 'Nntp-Posting-Host', ':', 'rac3.wam.umd.edu', 'Organization', ':', 'University', 'Maryland', ',', 'College', 'Park', 'Lines', ':', '15', 'I', 'wonder', 'anyone', 'could', 'enlighten', 'car', 'I', 'saw', 'day', '.', 'It', '2-door', 'sport', 'car', ',', 'look', 'late', '60s/', 'early', '70s', '.', 'It', 'call', 'Bricklin', '.', 'The', 'doors', 'really', 'small', '.', 'In', 'addition', ',', 'front', 'bumper', 'separate', 'rest', 'body', '.', 'This', 'I', 'know', '.', 'If', 'anyone', 'tellme', 'model', 'name', ',', 'engine', 'specs', ',', 'years', 'production', ',', 'car', 'make', ',', 'history', ',', 'whatever', 'info', 'funky', 'look', 'car', ',', 'please', 'e-mail', '.', 'Thanks', ',', '-', 'IL', '--', '--', 'bring', 'neighborhood', 'Lerxst', '--', '--']\n"
     ]
    }
   ],
   "source": [
    "from nltk. corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop=[x for x in lem if x not in stopwords.words('english')]\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjKQKWLGFQRl"
   },
   "source": [
    "- **Stemming (nltk):**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es un método para reducir la palabra a su raíz. El algoritmo más usado es el algoritmo de Porter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', ':', 'lerxst', '@', 'wam.umd.edu', '(', \"'s\", 'thing', ')', 'subject', ':', 'what', 'car', '!', '?', 'nntp-posting-host', ':', 'rac3.wam.umd.edu', 'organ', ':', 'univers', 'maryland', ',', 'colleg', 'park', 'line', ':', '15', 'I', 'wonder', 'anyon', 'could', 'enlighten', 'car', 'I', 'saw', 'day', '.', 'It', '2-door', 'sport', 'car', ',', 'look', 'late', '60s/', 'earli', '70', '.', 'It', 'call', 'bricklin', '.', 'the', 'door', 'realli', 'small', '.', 'In', 'addit', ',', 'front', 'bumper', 'separ', 'rest', 'bodi', '.', 'thi', 'I', 'know', '.', 'If', 'anyon', 'tellm', 'model', 'name', ',', 'engin', 'spec', ',', 'year', 'product', ',', 'car', 'make', ',', 'histori', ',', 'whatev', 'info', 'funki', 'look', 'car', ',', 'pleas', 'e-mail', '.', 'thank', ',', '-', 'IL', '--', '--', 'bring', 'neighborhood', 'lerxst', '--', '--']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "stem=[stemmer.stem(x) for x in stop]\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HNfMRbqmFSIR"
   },
   "source": [
    "- **Filtrado de palabras:**\n",
    "\n",
    "Removemos todo lo que no sean palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'lerxst', 'thing', 'subject', 'what', 'car', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'I', 'wonder', 'anyon', 'could', 'enlighten', 'car', 'I', 'saw', 'day', 'It', 'sport', 'car', 'look', 'late', 'earli', 'It', 'call', 'bricklin', 'the', 'door', 'realli', 'small', 'In', 'addit', 'front', 'bumper', 'separ', 'rest', 'bodi', 'thi', 'I', 'know', 'If', 'anyon', 'tellm', 'model', 'name', 'engin', 'spec', 'year', 'product', 'car', 'make', 'histori', 'whatev', 'info', 'funki', 'look', 'car', 'pleas', 'thank', 'IL', 'bring', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "alpha=[x for x in stem if x.isalpha()]\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOzGVenvIESr"
   },
   "source": [
    "### Preprocesamiento completo\n",
    "\n",
    "Utilizar o no cada uno de los métodos vistos es una decisión que dependerá del caso particular de aplicación. Para este ejercicio vamos a considerar las siguientes combinaciones:\n",
    "\n",
    "- Tokenización\n",
    "- Tokenización, Lematización, Stemming.\n",
    "- Tokenización, Stop Words.\n",
    "- Tokenización, Lematización, Stop Words, Stemming.\n",
    "- Tokenización, Lematización, Stop Words, Stemming, Filtrado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenización\n",
    "\n",
    "Armo un vector con cada articulo tokenizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PyC4gxFzRE0B"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mpier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "token =[word_tokenize(x) for x in twenty_train['data']]\n",
    "test_token =[word_tokenize(x) for x in twenty_test['data']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenización, Lematización, Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mpier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenLem=[[lemmatizer.lemmatize(x,pos='v') for x in y] for y in token ]\n",
    "test_tokenLem=[[lemmatizer.lemmatize(x,pos='v') for x in y] for y in test_token ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "tokenLemStem=[[stemmer.stem(x) for x in y] for y in tokenLem]\n",
    "test_tokenLemStem=[[stemmer.stem(x) for x in y] for y in test_tokenLem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenización, Stop Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mpier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk. corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "tokenStop=[[x for x in y if x not in stopwords.words('english')] for y in token]\n",
    "test_tokenStop=[[x for x in y if x not in stopwords.words('english')] for y in test_token]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenización, Lematización, Stop Words, Stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mpier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenStopLem=[[lemmatizer.lemmatize(x,pos='v') for x in y] for y in tokenStop ]\n",
    "test_tokenStopLem=[[lemmatizer.lemmatize(x,pos='v') for x in y] for y in test_tokenStop ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "tokenStopLemStem=[[stemmer.stem(x) for x in y] for y in tokenStopLem]\n",
    "test_tokenStopLemStem=[[stemmer.stem(x) for x in y] for y in test_tokenStopLem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenización, Lematización, Stop Words, Stemming, Filtrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenStopLemStemAlpha=[[x for x in y if x.isalpha()] for y in tokenStopLemStem ]\n",
    "test_tokenStopLemStemAlpha=[[x for x in y if x.isalpha()] for y in test_tokenStopLemStem ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "datosProcesados=[token,tokenLemStem,tokenStop,tokenStopLemStem,tokenStopLemStemAlpha]\n",
    "test_datosProcesados=[test_token,test_tokenLemStem,test_tokenStop,test_tokenStopLemStem,test_tokenStopLemStemAlpha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tosave=[datosProcesados,test_datosProcesados]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculo de longitudes promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenPromedio(x):\n",
    "    #calculo el pormedio de plabaras \n",
    "    temp=0\n",
    "    for i in x:\n",
    "        temp=temp+len(set(i))\n",
    "    return(temp/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180.95837016086264\n",
      "164.87661304578398\n",
      "149.42805373873077\n",
      "139.27320134346826\n",
      "102.65396853455896\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(itemlist[0])):\n",
    "    print(lenPromedio(itemlist[0][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wP3n8ZwFc7_D"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- Cómo cambia el tamaño del vocabulario al agregar Lematización y Stemming?\n",
    "- Cómo cambia el tamaño del vocabulario al Stop Words?\n",
    "- Analice muy brevemente ventajas y desventajas del tamaño del dataset en cada caso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TgrHtzEuIqid"
   },
   "source": [
    "### Guardado de pre procesamiento\n",
    "Vamos a guardar lo preprocesado usando pickle, que nos permite serializar objetos y guardarlos en disco, es muy importante que sepan hacer esto si no quieren perder tiempo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ofiOlGS4SsgL"
   },
   "outputs": [],
   "source": [
    "#Salvado del procesamiento a disco:\n",
    "'''\n",
    "import pickle\n",
    "\n",
    "with open('art_filt.pck', 'wb') as fp:\n",
    "    pickle.dump(tosave, fp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_duXZlcPSxTU"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open ('art_filt.pck', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-R-LPRoIV0E"
   },
   "source": [
    "## Vectorización de texto\n",
    "\n",
    "- **Obtención del vocabulario y obtención de la probabilidad**\n",
    "\n",
    "Como se vió en clase, los vectorizadores cuentan con dos parámetros de ajuste.\n",
    "\n",
    "- max_df: le asignamos una maxima frecuencia de aparición, eliminando las palabras comunes que no aportan información.\n",
    "\n",
    "- min_df: le asignamos la minima cantidad de veces que tiene que aparecer una palabra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4w4SCTsYaHF8"
   },
   "source": [
    "Al igual que con las diferentes opciones de preprocesamiento, lo mismo ocurre con la vectorización. Podemos utilizar CountVectorizer o TfidfVectorizer según el caso (con diferentes valores de max_df y min_df).\n",
    "Para este ejercicio deben utilizar ambos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C74USIEyS3B7"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#junto los arreglos de strings a un parrafo para cada combinacion\n",
    "\n",
    "dataToProces=[[' '.join(y)  for y in x ] for x in itemlist[0]]\n",
    "test_dataToProces=[[' '.join(y)  for y in x ] for x in itemlist[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZjO8CFPTUM6"
   },
   "outputs": [],
   "source": [
    "raw_data.toarray() #Es una sparse matrix, vamos a expandirla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3iqIoNyTTYVc"
   },
   "outputs": [],
   "source": [
    "raw_data.toarray()[0,:].argmax() #Veamos a qué palabra pertenece la máxima ocurrencia en el primer artíclo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7ob-IgsMRS7"
   },
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a4bVz9QwMupE"
   },
   "source": [
    "Primero deben separar correctamente el dataset para hacer validación del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5S9rAz2tNvkw"
   },
   "source": [
    "Y luego deben entrenar el modelo de NaiveBayes con el dataset de train.\n",
    "\n",
    "Deben utilizar un modelo de NaiveBayes Multinomial y de Bernoulli. Ambos modelos estan disponibles en sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mDUWaF5_VB4K"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "results=[]\n",
    "#results.append(['Escala de gris','Alpha','Grupo','Accuraci'])\n",
    "\n",
    "_max_df=[0.9,0.8,0.7,0.6]# max_df establece que si aparece en el 80% de los documentos no aporta información\n",
    "_min_df=[100,90,80,70]# min_df establece que si no aparece en por lo menos 100 documentos tampoco\n",
    "clf = [MultinomialNB(),BernoulliNB()]\n",
    "\n",
    "for i in range(2): #Barro las dos distribuciones\n",
    "    for j in range(len(dataToProces)):#Barro los distintos tipos de filtrados\n",
    "        for k in range(len(_max_df)):#Barro los max_df\n",
    "            for u in range(len(_min_df)):#Barro los min_df\n",
    "                for m in range(2):\n",
    "                    if(m==0):\n",
    "                        count_vect = CountVectorizer(max_df=_max_df[k],min_df=_min_df[u])\n",
    "                    else:\n",
    "                        count_vect = TfidfVectorizer(max_df=_max_df[k],min_df=_min_df[u])\n",
    "                            \n",
    "                    \n",
    "                    raw_data = count_vect.fit_transform(dataToProces[j])\n",
    "                    clf[i].fit(raw_data, twenty_train['target'])\n",
    "                    results.append([m,i,j,_max_df[k],_min_df[u],clf[i].score(raw_data, twenty_train['target'])])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# opening the csv file in 'w' mode \n",
    "file = open('resultadosTrain.csv', 'w', newline ='') \n",
    "  \n",
    "with file: \n",
    "    # identifying header   \n",
    "    header = ['CountVectorizer_0','Multinomial_0','Filtros','max_df','min_df','Accuraci'] \n",
    "    writer = csv.DictWriter(file, fieldnames = header) \n",
    "      \n",
    "    # writing data row-wise into the csv file \n",
    "    writer.writeheader() \n",
    "    for x in results:\n",
    "        writer.writerow({'CountVectorizer_0':x[0],\n",
    "                         'Multinomial_0':x[1],\n",
    "                         'Filtros':x[2],\n",
    "                         'max_df':x[3],\n",
    "                         'min_df':x[4],\n",
    "                         'Accuraci':x[5]}) \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "692nzR2JOV1F"
   },
   "source": [
    "Finalmente comprobar el accuracy en train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preguntas\n",
    "\n",
    "- Con que combinación de preprocesamiento obtuvo los mejores resultados? Explique por qué cree que fue así.\n",
    "\n",
    "- Con que modelo obtuvo los mejores resultados? Explique por qué cree que fue así."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance de los modelos\n",
    "\n",
    "En el caso anterior, para medir la cantidad de artículos clasiicados correctamente se utilizó el mismo subconjunto del dataset que se utilizó para entrenar.\n",
    "\n",
    "Esta medida no es una medida del todo útil, ya que lo que interesa de un clasificador es su capacidad de clasificación de datos que no fueron utilizados para entrenar. Es por eso que se pide, para el clasificador entrenado con el subconjunto de training, cual es el porcentaje de artículos del subconjunto de testing clasificados correctamente. Comparar con el porcentaje anterior y explicar las diferencias.\n",
    "\n",
    "Finalmente deben observar las diferencias y extraer conclusiones en base al accuracy obtenido, el preprocesamiento y vectorización utilizado y el modelo, para cada combinación de posibilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "results=[]\n",
    "#results.append(['Escala de gris','Alpha','Grupo','Accuraci'])\n",
    "\n",
    "_max_df=[0.9,0.8,0.7,0.6]# max_df establece que si aparece en el 80% de los documentos no aporta información\n",
    "_min_df=[100,90,80,70]# min_df establece que si no aparece en por lo menos 100 documentos tampoco\n",
    "clf = [MultinomialNB(),BernoulliNB()]\n",
    "\n",
    "for i in range(2): #Barro las dos distribuciones\n",
    "    for j in range(len(dataToProces)):#Barro los distintos tipos de filtrados\n",
    "        for k in range(len(_max_df)):#Barro los max_df\n",
    "            for u in range(len(_min_df)):#Barro los min_df\n",
    "                count_vect = CountVectorizer(max_df=_max_df[k],min_df=_min_df[u])\n",
    "                raw_data =count_vect.fit_transform(dataToProces[j])\n",
    "                test_raw_data=count_vect.transform(test_dataToProces[j])\n",
    "                clf[i].fit(raw_data, twenty_train['target'])\n",
    "                results.append([i,j,_max_df[k],_min_df[u],clf[i].score(test_raw_data, twenty_test['target'])])\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# opening the csv file in 'w' mode \n",
    "file = open('resultadosTest.csv', 'w', newline ='') \n",
    "  \n",
    "with file: \n",
    "    # identifying header   \n",
    "    header = ['Multinomial_0','Filtros','max_df','min_df','Accuraci'] \n",
    "    writer = csv.DictWriter(file, fieldnames = header) \n",
    "      \n",
    "    # writing data row-wise into the csv file \n",
    "    writer.writeheader() \n",
    "    for x in results:\n",
    "        writer.writerow({'Multinomial_0':x[0],\n",
    "                         'Filtros':x[1],\n",
    "                         'max_df':x[2],\n",
    "                         'min_df':x[3],\n",
    "                         'Accuraci':x[4]}) \n",
    "   \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOf-BcvpeqKp"
   },
   "source": [
    "Preguntas\n",
    "\n",
    "- El accuracy en el dataset de test es mayor o menor que en train? Explique por qué."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Ejercicio_Clasificación_Texto.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
